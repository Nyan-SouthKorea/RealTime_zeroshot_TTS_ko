{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/wavmark/__init__.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n",
      "/data2/iena/240924_openvoice/OpenVoice/capsule test/openvoice/api.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(ckpt_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      "OpenVoice version: v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating duration from bitrate, this may be inaccurate\n",
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/torch/functional.py:666: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789115370/work/aten/src/ATen/native/SpectralOps.cpp:873.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/melo/download_utils.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(ckpt_path, map_location=device)\n",
      "/tmp/ipykernel_2161587/3256642629.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  source_se = torch.load(f'checkpoints_v2/base_speakers/ses/{speaker_key}.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "지금 이 음성은 저번에 사용한 티티에스 싸이트를 통해서 생성한 음성이 아니고,\n",
      "샘플 음성을 제로샷으로 습득하여 로컬 티티에스 모델에서 만들어낸 목소리 입니다.\n",
      "목소리를 잘 따라하는 것을 볼 수 있습니다.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import ToneColorConverter\n",
    "from melo.api import TTS\n",
    "\n",
    "# 초기화\n",
    "ckpt_converter = 'checkpoints_v2/converter'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "output_dir = 'outputs_v2'\n",
    "\n",
    "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
    "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 톤 컬러 임베딩\n",
    "reference_speaker = 'sunhi_sample.mp3' # This is the voice you want to clone\n",
    "target_se, audio_name = se_extractor.get_se(reference_speaker, tone_color_converter, vad=False)\n",
    "\n",
    "\n",
    "# 생성\n",
    "texts = { 'KR': \"지금 이 음성은 저번에 사용한 티티에스 싸이트를 통해서 생성한 음성이 아니고, 샘플 음성을 제로샷으로 습득하여 로컬 티티에스 모델에서 만들어낸 목소리 입니다. 목소리를 잘 따라하는 것을 볼 수 있습니다.\"}\n",
    "\n",
    "src_path = f'{output_dir}/tmp.wav'\n",
    "\n",
    "# Speed is adjustable\n",
    "speed = 1.1\n",
    "\n",
    "for language, text in texts.items():\n",
    "    print(language)\n",
    "    model = TTS(language=language, device=device)\n",
    "    speaker_ids = model.hps.data.spk2id\n",
    "    \n",
    "    for speaker_key in speaker_ids.keys():\n",
    "        speaker_id = speaker_ids[speaker_key]\n",
    "        speaker_key = speaker_key.lower().replace('_', '-')\n",
    "        \n",
    "        source_se = torch.load(f'checkpoints_v2/base_speakers/ses/{speaker_key}.pth', map_location=device)\n",
    "        model.tts_to_file(text, speaker_id, src_path, speed=speed)\n",
    "        save_path = f'{output_dir}/output_v2_{speaker_key}.wav'\n",
    "\n",
    "        # Run the tone color converter\n",
    "        encode_message = \"@MyShell\"\n",
    "        tone_color_converter.convert(\n",
    "            audio_src_path=src_path, \n",
    "            src_se=source_se, \n",
    "            tgt_se=target_se, \n",
    "            output_path=save_path,\n",
    "            message=encode_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "캡슐화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import ToneColorConverter\n",
    "from melo.api import TTS\n",
    "\n",
    "class Custom_TTS:\n",
    "    def __init__(self, model_path='checkpoints_v2'):\n",
    "        '''\n",
    "        model_path: TTS를 위한 베이스 모델, 음성 변조를 위한 베이스 모델이 위치한 path\n",
    "        '''\n",
    "        print('다음 Repo.를 참조하여 개발한 모듈입니다: https://github.com/myshell-ai/OpenVoice')\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # cuda 확인\n",
    "        self.check_cuda()\n",
    "\n",
    "    def check_cuda(self):\n",
    "        '''cuda 환경 확인'''\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f'사용 환경(cude): {self.device}')\n",
    "\n",
    "    def set_model(self, language='KR'):\n",
    "        '''\n",
    "        모델 설정\n",
    "        language: 언어 입력(en-au, en-br, en-default, en-india, en-newest, en-us, es, fr, jp, kr, zh)\n",
    "        '''\n",
    "        self.language = language\n",
    "        \n",
    "        # 톤 변경 모델 로드\n",
    "        self.tone_color_converter = ToneColorConverter(f'{self.model_path}/converter/config.json', device=self.device)\n",
    "        self.tone_color_converter.load_ckpt(f'{self.model_path}/converter/checkpoint.pth')\n",
    "        print('톤 변경 모델 로드 완료')\n",
    "\n",
    "        # TTS 모델 선언\n",
    "        self.tts_model = TTS(language=self.language, device=self.device)\n",
    "        print('TTS 모델 로드 완료')\n",
    "\n",
    "        # 기본 화자 음성 임베딩: TTS 모델이 생성한 음성을 음색 변환 모델이 입력할 때, 원본 화자의 음색 정보를 제공함\n",
    "        speaker_ids = self.tts_model.hps.data.spk2id\n",
    "        for speaker_key in speaker_ids.keys():\n",
    "            self.speaker_id = speaker_ids[speaker_key]\n",
    "            speaker_key = speaker_key.lower().replace('_', '-')\n",
    "        self.source_se = torch.load(f'{self.model_path}/base_speakers/ses/{speaker_key}.pth', map_location=self.device)\n",
    "        print('기본 화자 음성 임베딩 완료')\n",
    "\n",
    "    def get_reference_speaker(self, speaker_path, vad=True):\n",
    "        '''\n",
    "        흉내낼 목소리를 입력해주는 함수. \n",
    "        - 논문 상 최소 44초 길이 이상의 음성을 넣으라고 되어있음\n",
    "        - base 목소리가 여자이기 때문에, 조금의 실험을 해본 결과 남자 목소리 보다는 여자 목소리를 더 잘 따라하는 경향을 보임\n",
    "        - 꼭 mp3일 필요 없고 갤럭시 휴대폰 기본 녹음 포맷인 m4a도 문제 없었음\n",
    "\n",
    "        path: 복사할 음성의 상대 경로를 입력\n",
    "        vad: 목소리 감지 기능 켜기/끄기. 켤 경우 음성 내에서 목소리가 있는 부분만 전처리 함\n",
    "        '''\n",
    "        # 톤 컬러 임베딩\n",
    "        self.target_se, audio_name = se_extractor.get_se(speaker_path, self.tone_color_converter, vad=vad)\n",
    "        print('목소리 톤 임베딩 완료')\n",
    "\n",
    "    def make_speech(self, text, output_path='output', speed=1.1):\n",
    "        '''\n",
    "        텍스트를 입력하면 TTS를 수행하는 함수. mp3를 생성하여 로컬에 저장함\n",
    "        text: 변환을 원하는 언어를 입력\n",
    "        output_path: TTS 결과물이 출력되는 경로\n",
    "        speed: 음성 재생 속도. 1.1이 자연스러운 것 같음\n",
    "        '''\n",
    "        # 경로 설정, 기존 파일 존재시 삭제, 폴더 생성\n",
    "        src_path = f'{output_path}/tmp.wav'\n",
    "        if os.path.exists(output_path):\n",
    "            shutil.rmtree(output_path)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # TTS 수행\n",
    "        self.tts_model.tts_to_file(text, self.speaker_id, src_path, speed=speed)\n",
    "\n",
    "        # 목소리 변조 수행\n",
    "        self.tone_color_converter.convert(audio_src_path=src_path, \n",
    "                                          src_se=self.source_se, \n",
    "                                          tgt_se=self.target_se, \n",
    "                                          output_path=f'{output_path}/result.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 Repo.를 참조하여 개발한 모듈입니다: https://github.com/myshell-ai/OpenVoice\n",
      "사용 환경(cude): cuda:0\n"
     ]
    }
   ],
   "source": [
    "tts_module = Custom_TTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/wavmark/__init__.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n",
      "/data2/iena/240924_openvoice/OpenVoice/capsule test/openvoice/api.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(ckpt_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      "톤 변경 모델 로드 완료\n",
      "TTS 모델 로드 완료\n",
      "기본 화자 음성 임베딩 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/melo/download_utils.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(ckpt_path, map_location=device)\n",
      "/tmp/ipykernel_2197675/3450542876.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.source_se = torch.load(f'{self.model_path}/base_speakers/ses/{speaker_key}.pth', map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "tts_module.set_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 128.512)]\n",
      "after vad: dur = 128.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/torch/functional.py:666: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789115370/work/aten/src/ATen/native/SpectralOps.cpp:873.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "목소리 톤 임베딩 완료\n"
     ]
    }
   ],
   "source": [
    "tts_module.get_reference_speaker(speaker_path='iena_sample.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "오늘 회의시간에 너무 졸아서 고수석님께 혼났다\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "tts_module.make_speech('오늘 회의시간에 너무 졸아서 고수석님께 혼났다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check point 다운로드 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip: 100%|██████████| 116M/116M [00:13<00:00, 9.05MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip 다운로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 125M/125M [00:00<00:00, 168MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip 압축 해제 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "url = \"https://myshell-public-repo-host.s3.amazonaws.com/openvoice/checkpoints_v2_0417.zip\"\n",
    "filename = \"checkpoints_v2_0417.zip\"\n",
    "extract_path = \"checkpoints_v2\"  # 압축을 풀 디렉토리\n",
    "\n",
    "# HTTP 응답에서 Content-Length(파일 크기) 가져오기\n",
    "response = requests.get(url, stream=True)\n",
    "total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "# 다운로드 진행을 표시하는 tqdm 설정\n",
    "with open(filename, \"wb\") as file, tqdm(\n",
    "    desc=filename,\n",
    "    total=total_size,\n",
    "    unit='B',\n",
    "    unit_scale=True,\n",
    "    unit_divisor=1024,\n",
    ") as bar:\n",
    "    for data in response.iter_content(chunk_size=1024):\n",
    "        file.write(data)\n",
    "        bar.update(len(data))\n",
    "\n",
    "print(f\"{filename} 다운로드 완료!\")\n",
    "\n",
    "# 압축 해제 진행률 표시\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    # 압축된 파일의 전체 크기를 계산\n",
    "    total_unzipped_size = sum((zinfo.file_size for zinfo in zip_ref.infolist()))\n",
    "\n",
    "    # 압축 해제 진행률을 표시하는 tqdm 설정\n",
    "    with tqdm(total=total_unzipped_size, unit='B', unit_scale=True, unit_divisor=1024, desc=\"Extracting\") as bar:\n",
    "        for zinfo in zip_ref.infolist():\n",
    "            extracted_file_path = zip_ref.extract(zinfo, extract_path)\n",
    "            # 압축 해제된 파일 크기만큼 진행률을 업데이트\n",
    "            bar.update(zinfo.file_size)\n",
    "\n",
    "print(f\"{filename} 압축 해제 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다운로드 모듈 캡슐화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip: 263B [00:00, 728kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip 다운로드 완료!\n",
      "압축 해제 문제 발생\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Down_and_extract:\n",
    "    def do(self, url, file_name, extract_path):\n",
    "        try:\n",
    "\n",
    "            # HTTP 응답에서 Content-Length(파일 크기) 가져오기\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "            # 다운로드 진행을 표시하는 tqdm 설정\n",
    "            with open(filename, \"wb\") as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    file.write(data)\n",
    "                    bar.update(len(data))\n",
    "\n",
    "            print(f\"{filename} 다운로드 완료!\")\n",
    "\n",
    "            # 압축 해제 진행률 표시\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                # 압축된 파일의 전체 크기를 계산\n",
    "                total_unzipped_size = sum((zinfo.file_size for zinfo in zip_ref.infolist()))\n",
    "\n",
    "                # 압축 해제 진행률을 표시하는 tqdm 설정\n",
    "                with tqdm(total=total_unzipped_size, unit='B', unit_scale=True, unit_divisor=1024, desc=\"Extracting\") as bar:\n",
    "                    for zinfo in zip_ref.infolist():\n",
    "                        extracted_file_path = zip_ref.extract(zinfo, extract_path)\n",
    "                        # 압축 해제된 파일 크기만큼 진행률을 업데이트\n",
    "                        bar.update(zinfo.file_size)\n",
    "            print(f\"{filename} 압축 해제 완료!\")\n",
    "            return True\n",
    "        except:\n",
    "            print('압축 해제 문제 발생')\n",
    "            return False\n",
    "\n",
    "url = \"https://myshell-public-repo-host.s3.amazonaws.com/openvoice/checkpoints_v2_0417.zip\"\n",
    "filename = \"checkpoints_v2_0417.zip\"\n",
    "extract_path = \"checkpoints_v2\"  # 압축을 풀 디렉토리\n",
    "\n",
    "down_load = Down_and_extract()\n",
    "down_load.do(url, filename, extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 통합 모듈 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import ToneColorConverter\n",
    "from melo.api import TTS\n",
    "\n",
    "\n",
    "class Custom_TTS:\n",
    "    def __init__(self, model_path='checkpoints_v2'):\n",
    "        '''\n",
    "        model_path: TTS를 위한 베이스 모델, 음성 변조를 위한 베이스 모델이 위치한 path\n",
    "        '''\n",
    "        print('본 코드를 개발한 Repo. 입니다: https://github.com/Nyan-SouthKorea/RealTime_zeroshot_TTS_ko')\n",
    "        print('다음 Repo.를 참조하여 개발한 모듈입니다: https://github.com/myshell-ai/OpenVoice')\n",
    "        self.model_path = model_path\n",
    "\n",
    "        # cuda 확인\n",
    "        self.check_cuda()\n",
    "\n",
    "    def check_cuda(self):\n",
    "        '''cuda 환경 확인'''\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f'사용 환경(cude): {self.device}')\n",
    "\n",
    "    def checkpoint_download(self):\n",
    "        '''\n",
    "        모델의 pre-trained checkpoint가 있는지 확인하고 없으면 다운로드 함\n",
    "        - 모델의 폴더만 확인하기 때문에, 폴더 안에 모델 변경이 있어도 유효성 검사를 수행하지 않음\n",
    "        - 단순히 폴더가 없으면 다시 다운로드 하는 로직임\n",
    "        '''\n",
    "        if os.path.exists(self.model_path) == False:\n",
    "            download = Down_and_extract()\n",
    "            ret = download.do(url=\"https://myshell-public-repo-host.s3.amazonaws.com/openvoice/checkpoints_v2_0417.zip\", filename=\"checkpoints_v2_0417.zip\")\n",
    "            if ret == False:\n",
    "                with open('./error_txt.txt', 'r', encoding='utf-8-sig') as f:\n",
    "                    error_txt = f.read()\n",
    "                    print(error_txt)\n",
    "\n",
    "    def set_model(self, language='KR'):\n",
    "        '''\n",
    "        모델 설정\n",
    "        language: 언어 입력(en-au, en-br, en-default, en-india, en-newest, en-us, es, fr, jp, kr, zh)\n",
    "        '''\n",
    "        self.language = language\n",
    "        \n",
    "        # pre-trained 모델 다운로드\n",
    "        self.checkpoint_download()\n",
    "\n",
    "        # 톤 변경 모델 로드\n",
    "        self.tone_color_converter = ToneColorConverter(f'{self.model_path}/converter/config.json', device=self.device)\n",
    "        self.tone_color_converter.load_ckpt(f'{self.model_path}/converter/checkpoint.pth')\n",
    "        print('톤 변경 모델 로드 완료')\n",
    "\n",
    "        # TTS 모델 선언\n",
    "        self.tts_model = TTS(language=self.language, device=self.device)\n",
    "        print('TTS 모델 로드 완료')\n",
    "\n",
    "        # 기본 화자 음성 임베딩: TTS 모델이 생성한 음성을 음색 변환 모델이 입력할 때, 원본 화자의 음색 정보를 제공함\n",
    "        speaker_ids = self.tts_model.hps.data.spk2id\n",
    "        for speaker_key in speaker_ids.keys():\n",
    "            self.speaker_id = speaker_ids[speaker_key]\n",
    "            speaker_key = speaker_key.lower().replace('_', '-')\n",
    "        self.source_se = torch.load(f'{self.model_path}/base_speakers/ses/{speaker_key}.pth', map_location=self.device)\n",
    "        print('기본 화자 음성 임베딩 완료')\n",
    "\n",
    "    def get_reference_speaker(self, speaker_path, vad=True):\n",
    "        '''\n",
    "        흉내낼 목소리를 입력해주는 함수. \n",
    "        - 논문 상 최소 44초 길이 이상의 음성을 넣으라고 되어있음\n",
    "        - base 목소리가 여자이기 때문에, 조금의 실험을 해본 결과 남자 목소리 보다는 여자 목소리를 더 잘 따라하는 경향을 보임\n",
    "        - 꼭 mp3일 필요 없고 갤럭시 휴대폰 기본 녹음 포맷인 m4a도 문제 없었음\n",
    "\n",
    "        path: 복사할 음성의 상대 경로를 입력\n",
    "        vad: 목소리 감지 기능 켜기/끄기. 켤 경우 음성 내에서 목소리가 있는 부분만 전처리 함\n",
    "        '''\n",
    "        # 톤 컬러 임베딩\n",
    "        self.target_se, audio_name = se_extractor.get_se(speaker_path, self.tone_color_converter, vad=vad)\n",
    "        print('목소리 톤 임베딩 완료')\n",
    "\n",
    "    def make_speech(self, text, output_path='output', speed=1.1):\n",
    "        '''\n",
    "        텍스트를 입력하면 TTS를 수행하는 함수. mp3를 생성하여 로컬에 저장함\n",
    "        text: 변환을 원하는 언어를 입력\n",
    "        output_path: TTS 결과물이 출력되는 경로\n",
    "        speed: 음성 재생 속도. 1.1이 자연스러운 것 같음\n",
    "        '''\n",
    "        # 경로 설정, 기존 파일 존재시 삭제, 폴더 생성\n",
    "        src_path = f'{output_path}/tmp.wav'\n",
    "        if os.path.exists(output_path):\n",
    "            shutil.rmtree(output_path)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # TTS 수행\n",
    "        self.tts_model.tts_to_file(text, self.speaker_id, src_path, speed=speed)\n",
    "\n",
    "        # 목소리 변조 수행\n",
    "        self.tone_color_converter.convert(audio_src_path=src_path, \n",
    "                                          src_se=self.source_se, \n",
    "                                          tgt_se=self.target_se, \n",
    "                                          output_path=f'{output_path}/result.wav')\n",
    "\n",
    "class Down_and_extract:\n",
    "    def do(self, url, filename):\n",
    "        try:\n",
    "            # HTTP 응답에서 Content-Length(파일 크기) 가져오기\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "            # 다운로드 진행을 표시하는 tqdm 설정\n",
    "            with open(filename, \"wb\") as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    file.write(data)\n",
    "                    bar.update(len(data))\n",
    "\n",
    "            print(f\"{filename} 다운로드 완료!\")\n",
    "\n",
    "            # 압축 해제 진행률 표시\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                # 압축된 파일의 전체 크기를 계산\n",
    "                total_unzipped_size = sum((zinfo.file_size for zinfo in zip_ref.infolist()))\n",
    "\n",
    "                # 압축 해제 진행률을 표시하는 tqdm 설정\n",
    "                with tqdm(total=total_unzipped_size, unit='B', unit_scale=True, unit_divisor=1024, desc=\"Extracting\") as bar:\n",
    "                    for zinfo in zip_ref.infolist():\n",
    "                        extracted_file_path = zip_ref.extract(zinfo, './')\n",
    "                        # 압축 해제된 파일 크기만큼 진행률을 업데이트\n",
    "                        bar.update(zinfo.file_size)\n",
    "            print(f\"{filename} 압축 해제 완료!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f'압축 해제 문제 발생: \\n{e}')\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 코드를 개발한 Repo. 입니다: https://github.com/Nyan-SouthKorea/RealTime_zeroshot_TTS_ko\n",
      "다음 Repo.를 참조하여 개발한 모듈입니다: https://github.com/myshell-ai/OpenVoice\n",
      "사용 환경(cude): cuda:0\n"
     ]
    }
   ],
   "source": [
    "tts_module = Custom_TTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip: 100%|██████████| 116M/116M [00:14<00:00, 8.15MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip 다운로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 125M/125M [00:00<00:00, 180MB/s]\n",
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints_v2_0417.zip 압축 해제 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/wavmark/__init__.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n",
      "/data2/iena/240924_openvoice/OpenVoice/capsule test/openvoice/api.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(ckpt_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints_v2/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      "톤 변경 모델 로드 완료\n",
      "TTS 모델 로드 완료\n",
      "기본 화자 음성 임베딩 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/melo/download_utils.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(ckpt_path, map_location=device)\n",
      "/tmp/ipykernel_2261619/1160608394.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.source_se = torch.load(f'{self.model_path}/base_speakers/ses/{speaker_key}.pth', map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "tts_module.set_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/openvoice/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[(0.0, 128.512)]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "after vad: dur = 128.512\n",
      "목소리 톤 임베딩 완료\n"
     ]
    }
   ],
   "source": [
    "tts_module.get_reference_speaker(speaker_path='iena_sample.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text split to sentences.\n",
      "오늘 회의시간에 너무 졸아서 고수석님께 혼났다\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n"
     ]
    }
   ],
   "source": [
    "tts_module.make_speech('오늘 회의시간에 너무 졸아서 고수석님께 혼났다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
